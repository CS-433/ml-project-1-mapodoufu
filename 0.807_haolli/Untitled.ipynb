{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43a0e105-9291-41c3-b219-37b968b654f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\ADA\\lib\\site-packages\\ipykernel_launcher.py:203: RuntimeWarning: overflow encountered in exp\n",
      "E:\\Anaconda\\envs\\ADA\\lib\\site-packages\\ipykernel_launcher.py:193: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# imports and useful start lines\n",
    "#####################\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#####################\n",
    "# Algorithms\n",
    "#####################\n",
    "# Linear gradient descent\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        w: numpy array of shape=(D,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    e = y - np.matmul(tx, w)\n",
    "    te = np.transpose(e)\n",
    "    N = len(y)\n",
    "    return (np.matmul(te, e) / (2 * N))\n",
    "\n",
    "def compute_loss_mae(y, tx, w):\n",
    "    e = y - np.matmul(tx, w)\n",
    "    N = len(y)\n",
    "    return np.mean(np.abs(e))\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N, D)\n",
    "        w: numpy array of shape=(D, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (D, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - np.matmul(tx, w)\n",
    "    N = len(y)\n",
    "    return np.matmul(tx.transpose(), e) * (-1) / N\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: final optimized parameter w of shape(D, )\n",
    "        loss: final loss (scalar) at max_iter with final optimized parameter w\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "# Stochastic gradient descent for linear models\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w): \n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(batch_size=1, )\n",
    "        tx: numpy array of shape=(batch_size=1, D)\n",
    "        w: numpy array of shape=(D, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (D, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(y)\n",
    "    e = y - np.matmul(tx, w)\n",
    "    return np.matmul(tx.transpose(), e) * (-1) / batch_size\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: final optimized parameter w of shape(D, ) (using SGD)\n",
    "        loss: final loss (scalar) at max_iter with final optimized parameter w (using SGD)\n",
    "    \"\"\"\n",
    "\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size = batch_size): # there is only one batch actually.\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "# Least square regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "\n",
    "    >>> least_squares(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]))\n",
    "    (array([ 0.21212121, -0.12121212]), 8.666684749742561e-33)\n",
    "    \"\"\"\n",
    "    XTX = tx.transpose() @ tx\n",
    "    XTy = tx.transpose() @ y\n",
    "    #w = np.linalg.solve(XTX, XTy) we encountered singular matrices in real practice. do the below:\n",
    "    pinv = np.linalg.pinv(XTX)\n",
    "    w = pinv @ XTy\n",
    "    mse = compute_loss(y, tx, w)\n",
    "    return w, mse\n",
    "\n",
    "# Ridge regression\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 0)\n",
    "    array([ 0.21212121, -0.12121212])\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 1)\n",
    "    array([0.03947092, 0.00319628])\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    D = tx.shape[1]\n",
    "\n",
    "    # Generating the diagnoal matrix lambda' * I (D * D) = Lambda_slash\n",
    "    lambda_slash = 2 * lambda_ * N\n",
    "    Lambda_slash = np.diag(lambda_slash * np.ones(D))\n",
    "\n",
    "    Coef = (tx.transpose() @ tx) + Lambda_slash\n",
    "    w_star = np.linalg.solve(Coef, tx.transpose() @ y)\n",
    "    loss = compute_loss(y, tx, w_star)\n",
    "    return w_star, loss\n",
    "\n",
    "\n",
    "# Logistic regression \n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w): # Note this is different from compute_loss from previous content\n",
    "    assert y.shape[0] == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    " \n",
    "    N = y.shape[0]\n",
    "    res = 0\n",
    "    for n in range(0, N):\n",
    "        xnt_w = tx[n].transpose() @ w\n",
    "        res += np.log(1 + np.exp(xnt_w)) - y[n] * xnt_w\n",
    "    res = res / N\n",
    "    return np.squeeze(res)\n",
    "\n",
    "def calculate_gradient(y, tx, w): # Note this is different from the compute_gradient in the previous content.\n",
    "    N = y.shape[0]\n",
    "    return (tx.transpose() @ (sigmoid(tx @ w) - y)) / N\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma): # Do one step of gradient descent using logistic regression. \n",
    "    w_descent = w - gamma * calculate_gradient(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w) # we are here calculating the loss corresponding the w before update.\n",
    "    return loss, w_descent\n",
    "\n",
    "def logistic_regression_gradient_descent(y, tx, initial_w, max_iter, gamma): # Do the iterative descent\n",
    "    # init parameters\n",
    "    losses = 0\n",
    "    w = initial_w\n",
    "    for iter in range(max_iter):\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    return w, loss\n",
    "\n",
    "# Regularized logistic regression\n",
    "def penalized_logistic_regression(y, tx, w, lambda_): # Return the penalized loss and gradient\n",
    "    loss_penalized = np.squeeze(calculate_loss(y, tx, w) + lambda_ * (w.transpose() @ w))\n",
    "    gradient_penalized = calculate_gradient(y, tx, w) + (2 * lambda_ * w)\n",
    "    return loss_penalized, gradient_penalized\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_): # Do one step of penalized logistic regression gd.\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression_penalized_gradient_descent(y, tx, lambda_, initial_w, max_iter, gamma): # Do the iterative descent\n",
    "    w = initial_w\n",
    "    loss = 0\n",
    "    for iter in range(max_iter):\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    return w, loss\n",
    "\n",
    "# transformationï¼š regression -> classification\n",
    "def predict_labels(w, tx):\n",
    "    y_pred = tx @ w\n",
    "    y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    return y_pred\n",
    "\n",
    "# Build polynomial features\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"Polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    rows, cols = x.shape\n",
    "\n",
    "    phi = np.zeros((rows, cols*(degree)))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            phi[i][j] = x[i][j]\n",
    "\n",
    "    for d in range(2, degree+1):\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                phi[i][j + (d-1)*cols] = x[i][j]**d\n",
    "\n",
    "    return phi\n",
    "\n",
    "#######################\n",
    "# Writing the results\n",
    "#######################\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w', newline = '') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            if int(r2) == 0:\n",
    "                r2 = -1\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "\n",
    "########################\n",
    "# Loading the data\n",
    "########################\n",
    "def load_data(data_path):\n",
    "    idx = np.genfromtxt(data_path, delimiter=\",\", skip_header = 1, usecols=[0])\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", dtype=str, skip_header = 1, usecols=[1], converters={1:lambda x: 0 if x == b'b' else 1})\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header = 1, usecols = list(range(2,32)))\n",
    "    return y, x, idx\n",
    "\n",
    "data_folder = \"Data\\\\\"\n",
    "y, tx_train, idx_train = load_data(data_folder + \"train.csv\")\n",
    "_, tx_test, idx_test = load_data(data_folder + \"test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# Cleaning the data\n",
    "#################\n",
    "\n",
    "# remove related features\n",
    "def remove_related_features(tx):\n",
    "    del_features = [5, 6, 12, 21, 24, 25, 26, 27, 28, 29]\n",
    "    tx = np.delete(tx, del_features, 1)\n",
    "    return tx\n",
    "\n",
    "tx_train = remove_related_features(tx_train)\n",
    "tx_test = remove_related_features(tx_test)\n",
    "\n",
    "# spliting the data\n",
    "def groupby_jetnum(tx): # Group by input data (related features removed) based on pri_jet_num\n",
    "    group0 = (tx[:, 18] == 0)\n",
    "    group1 = (tx[:, 18] == 1)\n",
    "    group2 = (tx[:, 18] != 0) & (tx[:, 18] != 1)\n",
    "    # for index 18, see imp_draft.ipynb\n",
    "    return [group0, group1, group2]\n",
    "\n",
    "groups_train = groupby_jetnum(tx_train)\n",
    "groups_test = groupby_jetnum(tx_test)\n",
    "\n",
    "# replacing na values with column mean\n",
    "def replace_na_values(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        msk = (data[:, i] != -999.)\n",
    "        # Replace NA values with mean value\n",
    "        median = np.median(data[msk, i])\n",
    "        if np.isnan(median):\n",
    "            median = 0\n",
    "        data[~msk, i] = median\n",
    "    return data\n",
    "\n",
    "#tx_train = replace_na_values(tx_train)\n",
    "#tx_test = replace_na_values(tx_test)\n",
    "# It's better to deal with NAs within of subgroups below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# Training model\n",
    "################\n",
    "degree = 4\n",
    "lambda_ = 1e-5\n",
    "preds = np.zeros(tx_test.shape[0])\n",
    "for i in range(len(groups_train)):\n",
    "    feature_tr = tx_train[groups_train[i]]\n",
    "    label_tr = y[groups_train[i]]\n",
    "    feature_te = tx_test[groups_test[i]]\n",
    "    \n",
    "    feature_tr = replace_na_values(feature_tr)\n",
    "    feature_te = replace_na_values(feature_te)\n",
    "    \n",
    "    phi_tr = build_poly(feature_tr, degree)\n",
    "    phi_te = build_poly(feature_te, degree)\n",
    "        \n",
    "    #w_star, _ = ridge_regression(label_tr, phi_tr, lambda_) #<---ridge regression\n",
    "    \n",
    "    #w_star, _ = least_squares(label_tr, phi_tr) <--------- least squares\n",
    "    \n",
    "    #w_init = np.zeros(feature_tr.shape[1])\n",
    "    #w_star, _ = least_squares_GD(label_tr, feature_tr, w_init, 50, 10e-10) <------------- gradient descent\n",
    "    \n",
    "    w_init = np.zeros(phi_tr.shape[1])\n",
    "    w_star, _ = logistic_regression_penalized_gradient_descent(label_tr, phi_tr, 1e-10, w_init, 50, 1e-10) #<---regularized logistic\n",
    "    \n",
    "    #w_init = np.zeros(phi_tr.shape[1])\n",
    "    #w_star, _ = logistic_regression_gradient_descent(label_tr, phi_tr, w_init, 50, 1e-10)  #<----------- logistic\n",
    "\n",
    "    #pred_y = predict_labels(w_star, phi_te) #<---all but logistic\n",
    "    \n",
    "    pred_y = sigmoid(phi_te @ w_star)\n",
    "    pred_y[np.where(pred_y <= 0.5)] = 0\n",
    "    pred_y[np.where(pred_y > 0.5)] = 1  #<---(regularized) logistic\n",
    "    \n",
    "    preds[groups_test[i]] = pred_y \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14cf5075-620e-4ad5-b6a5-126a4ca1f0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efebde31-d9f9-4c7c-8032-d8fbae886e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = data_folder + 'submission_logistic_regularized_poly_remove_corr_column.csv'\n",
    "create_csv_submission(idx_test, preds, OUTPUT_PATH)\n",
    "print('Submission file created!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
