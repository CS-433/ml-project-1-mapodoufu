{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d6b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# imports\n",
    "#########\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c715a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Loading the data\n",
    "#########\n",
    "data_file = \"Data\\\\\"\n",
    "data_name = \"train.csv\"\n",
    "\n",
    "idx = np.genfromtxt(data_file + data_name, delimiter=\",\", skip_header = 1, usecols=[0])\n",
    "y = np.genfromtxt(data_file + data_name, delimiter=\",\", dtype=str, skip_header = 1, usecols=[1], converters={1:lambda x: 0 if x == b'b' else 1})\n",
    "tx = np.genfromtxt(data_file + data_name, delimiter=\",\", skip_header = 1, usecols = list(range(2,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4540c3d-314b-49be-8732-111532dde2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file + data_name)\n",
    "df.columns[2:].values == 'PRI_jet_num'\n",
    "# this is the original input. after deleting relating columns, 22 becomes 18 (delete col 5, 6, 12, 21 ahead of 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "000f2b52-999c-435d-b1d9-3961b55e7e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>71.989</td>\n",
       "      <td>36.548</td>\n",
       "      <td>5.042</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.392</td>\n",
       "      <td>5.042</td>\n",
       "      <td>55.892</td>\n",
       "      <td>...</td>\n",
       "      <td>2.859</td>\n",
       "      <td>144.665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>58.179</td>\n",
       "      <td>68.083</td>\n",
       "      <td>22.439</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.585</td>\n",
       "      <td>22.439</td>\n",
       "      <td>50.618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>80.408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>105.457</td>\n",
       "      <td>60.526</td>\n",
       "      <td>75.839</td>\n",
       "      <td>39.757</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.390</td>\n",
       "      <td>22.183</td>\n",
       "      <td>120.462</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.890</td>\n",
       "      <td>198.907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.992</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>41.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>94.951</td>\n",
       "      <td>19.362</td>\n",
       "      <td>68.812</td>\n",
       "      <td>13.504</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.365</td>\n",
       "      <td>13.504</td>\n",
       "      <td>55.859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811</td>\n",
       "      <td>112.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>-999.000</td>\n",
       "      <td>72.756</td>\n",
       "      <td>70.831</td>\n",
       "      <td>7.479</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.025</td>\n",
       "      <td>7.479</td>\n",
       "      <td>83.240</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.596</td>\n",
       "      <td>99.405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2       3       4        5        6      7   \\\n",
       "0       138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064   \n",
       "1       160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   \n",
       "2      -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   \n",
       "3       143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   \n",
       "4       175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891   \n",
       "...         ...      ...      ...     ...     ...      ...      ...    ...   \n",
       "249995 -999.000   71.989   36.548   5.042 -999.00 -999.000 -999.000  1.392   \n",
       "249996 -999.000   58.179   68.083  22.439 -999.00 -999.000 -999.000  2.585   \n",
       "249997  105.457   60.526   75.839  39.757 -999.00 -999.000 -999.000  2.390   \n",
       "249998   94.951   19.362   68.812  13.504 -999.00 -999.000 -999.000  3.365   \n",
       "249999 -999.000   72.756   70.831   7.479 -999.00 -999.000 -999.000  2.025   \n",
       "\n",
       "            8        9   ...     20       21   22       23       24       25  \\\n",
       "0       41.928  197.760  ... -0.277  258.733  2.0   67.435    2.150    0.444   \n",
       "1        2.078  125.157  ... -1.916  164.546  1.0   46.226    0.725    1.158   \n",
       "2        9.336  197.814  ... -2.186  260.414  1.0   44.251    2.053   -2.028   \n",
       "3        0.414   75.968  ...  0.060   86.062  0.0 -999.000 -999.000 -999.000   \n",
       "4       16.405   57.983  ... -0.871   53.131  0.0 -999.000 -999.000 -999.000   \n",
       "...        ...      ...  ...    ...      ...  ...      ...      ...      ...   \n",
       "249995   5.042   55.892  ...  2.859  144.665  0.0 -999.000 -999.000 -999.000   \n",
       "249996  22.439   50.618  ... -0.867   80.408  0.0 -999.000 -999.000 -999.000   \n",
       "249997  22.183  120.462  ... -2.890  198.907  1.0   41.992    1.800   -0.166   \n",
       "249998  13.504   55.859  ...  0.811  112.718  0.0 -999.000 -999.000 -999.000   \n",
       "249999   7.479   83.240  ... -1.596   99.405  0.0 -999.000 -999.000 -999.000   \n",
       "\n",
       "             26      27       28       29  \n",
       "0        46.062    1.24   -2.475  113.497  \n",
       "1      -999.000 -999.00 -999.000   46.226  \n",
       "2      -999.000 -999.00 -999.000   44.251  \n",
       "3      -999.000 -999.00 -999.000    0.000  \n",
       "4      -999.000 -999.00 -999.000    0.000  \n",
       "...         ...     ...      ...      ...  \n",
       "249995 -999.000 -999.00 -999.000    0.000  \n",
       "249996 -999.000 -999.00 -999.000    0.000  \n",
       "249997 -999.000 -999.00 -999.000   41.992  \n",
       "249998 -999.000 -999.00 -999.000    0.000  \n",
       "249999 -999.000 -999.00 -999.000    0.000  \n",
       "\n",
       "[250000 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_related_features(tx):\n",
    "    del_features = [5, 6, 12, 21, 24, 25, 26, 27, 28, 29]\n",
    "    tx = np.delete(tx, del_features, 1)\n",
    "    return tx\n",
    "tx_remove = remove_related_features(tx)\n",
    "pd.DataFrame(tx_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0117504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Defining the loss functions\n",
    "#########\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        w: numpy array of shape=(D,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    e = y - np.matmul(tx, w)\n",
    "    te = np.transpose(e)\n",
    "    N = len(y)\n",
    "    return (np.matmul(te, e) / (2 * N))\n",
    "\n",
    "def compute_loss_mae(y, tx, w):\n",
    "    e = y - np.matmul(tx, w)\n",
    "    N = len(y)\n",
    "    return np.mean(np.abs(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "03e71cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Linear Gradient Descent\n",
    "##########\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N, D)\n",
    "        w: numpy array of shape=(D, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (D, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - np.matmul(tx, w)\n",
    "    N = len(y)\n",
    "    return np.matmul(tx.transpose(), e) * (-1) / N\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: final optimized parameter w of shape(D, )\n",
    "        loss: final loss (scalar) at max_iter with final optimized parameter w\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1611e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Stochastic Gradient Descent\n",
    "#################\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w): \n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(batch_size=1, )\n",
    "        tx: numpy array of shape=(batch_size=1, D)\n",
    "        w: numpy array of shape=(D, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (D, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = len(y)\n",
    "    e = y - np.matmul(tx, w)\n",
    "    return np.matmul(tx.transpose(), e) * (-1) / batch_size\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D)\n",
    "        initial_w: numpy array of shape=(D, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: final optimized parameter w of shape(D, ) (using SGD)\n",
    "        loss: final loss (scalar) at max_iter with final optimized parameter w (using SGD)\n",
    "    \"\"\"\n",
    "\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size = batch_size): # there is only one batch actually.\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d9ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Least square regression using normal equations\n",
    "##############\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "\n",
    "    >>> least_squares(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]))\n",
    "    (array([ 0.21212121, -0.12121212]), 8.666684749742561e-33)\n",
    "    \"\"\"\n",
    "    XTX = tx.transpose() @ tx\n",
    "    XTy = tx.transpose() @ y\n",
    "    w = np.linalg.solve(XTX, XTy)\n",
    "    mse = compute_loss(y, tx, w)\n",
    "    return w, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aacc297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Ridge regression\n",
    "################\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 0)\n",
    "    array([ 0.21212121, -0.12121212])\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 1)\n",
    "    array([0.03947092, 0.00319628])\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    D = tx.shape[1]\n",
    "\n",
    "    # Generating the diagnoal matrix lambda' * I (D * D) = Lambda_slash\n",
    "    lambda_slash = 2 * lambda_ * N\n",
    "    Lambda_slash = np.diag(lambda_slash * np.ones(D))\n",
    "\n",
    "    Coef = (tx.transpose() @ tx) + Lambda_slash\n",
    "    w_star = np.linalg.inv(Coef) @ tx.transpose() @ y\n",
    "    loss = compute_loss(y, tx, w_star)\n",
    "    return w_star, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592df93b",
   "metadata": {},
   "source": [
    "### logistic regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e45731",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Logistic regression \n",
    "####################\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "\n",
    "    >>> sigmoid(np.array([0.1]))\n",
    "    array([0.52497919])\n",
    "    >>> sigmoid(np.array([0.1, 0.1]))\n",
    "    array([0.52497919, 0.52497919])\n",
    "    \"\"\"\n",
    "    e_pow = np.exp(t)\n",
    "    return e_pow / (1 + e_pow)\n",
    "\n",
    "# Note this is different from compute_loss from previous content\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(4).reshape(2, 2)\n",
    "    >>> w = np.c_[[2., 3.]]\n",
    "    >>> round(calculate_loss(y, tx, w), 8)\n",
    "    1.52429481\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y.shape[0] == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    " \n",
    "    N = y.shape[0]\n",
    "    res = 0\n",
    "    for n in range(0, N):\n",
    "        xnt_w = tx[n].transpose() @ w\n",
    "        res += np.log(1 + np.exp(xnt_w)) - y[n] * xnt_w\n",
    "    res = res / N\n",
    "    return res[0]\n",
    "\n",
    "# Note this is different from the compute_gradient in the previous content.\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "    \n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> calculate_gradient(y, tx, w)\n",
    "    array([[-0.10370763],\n",
    "           [ 0.2067104 ],\n",
    "           [ 0.51712843]])\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    return (tx.transpose() @ (sigmoid(tx @ w) - y)) / N\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression. Return the loss and the updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "        gamma: float\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1) \n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> w\n",
    "    array([[0.11037076],\n",
    "           [0.17932896],\n",
    "           [0.24828716]])\n",
    "    \"\"\"\n",
    "    w_descent = w - gamma * calculate_gradient(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w) # we are here calculating the loss corresponding the w before update.\n",
    "    return loss, w_descent\n",
    "\n",
    "def logistic_regression_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.5\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return w, loss[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c37f1",
   "metadata": {},
   "source": [
    "### regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6c3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.63537268\n",
    "    >>> gradient \n",
    "    array([[-0.08370763],\n",
    "           [ 0.2467104 ],\n",
    "           [ 0.57712843]])\n",
    "    \"\"\"\n",
    "    loss_penalized = calculate_loss(y, tx, w) + lambda_ * (w.transpose() @ w)[0][0]\n",
    "    gradient_penalized = calculate_gradient(y, tx, w) + (2 * lambda_ * w)\n",
    "    return loss_penalized, gradient_penalized\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: scalar\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.63537268\n",
    "    >>> w\n",
    "    array([[0.10837076],\n",
    "           [0.17532896],\n",
    "           [0.24228716]])\n",
    "    \"\"\"\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression_penalized_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.5\n",
    "    lambda_ = 0.0005\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return w, loss[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
